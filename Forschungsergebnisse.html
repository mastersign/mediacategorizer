<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <title>Forschungsergebnisse - OLL - Verschriftlichung von Videosequenzen</title>
    <meta name="author" content="Tobias Kiertscher" />
    <meta name="date" content="31.01.2014" />
    
    <style type="text/css">
html, body, div, span, object, iframe,
h1, h2, h3, h4, h5, h6, p, blockquote, pre,
abbr, address, cite, code,
del, dfn, em, img, ins, kbd, q, samp,
small, strong, sub, sup, var,
b, i,
dl, dt, dd, ol, ul, li,
fieldset, form, label, legend,
table, caption, tbody, tfoot, thead, tr, th, td,
article, aside, canvas, details, figcaption, figure,
footer, header, hgroup, menu, nav, section, summary,
time, mark, audio, video
{
    margin: 0;
    padding: 0;
    border: 0;
    outline: 0;
    font-size: 100%;
    vertical-align: baseline;
    background: transparent;
    color: inherit;
    font-family: inherit;
}

body
{
    line-height: 100%;
    background-color: #fff;
    color: #000;
    font-family: sans-serif;
    text-align: left;
}

h1, h2, h3, h4, h5, h6
{
    font-family: serif;
}

article, aside, details, figcaption, figure,
footer, header, hgroup, menu, nav, section
{
    display: block;
}

nav ul
{
    list-style: none;
}

blockquote, q
{
    quotes: none;
}

blockquote:before, blockquote:after,
q:before, q:after
{
    content: '';
    content: none;
}

code
{
    font-family: monospace;
}

ins
{
    text-decoration: underline;
}

mark
{
    font-style: italic;
    font-weight: bold;
}

del
{
    text-decoration: line-through;
}

a
{
    margin: 0;
    padding: 0;
    font-size: 100%;
    vertical-align: baseline;
    background: transparent;
}

abbr[title], dfn[title]
{
    border-bottom: 1px dotted;
    cursor: help;
}

table
{
    border-collapse: collapse;
    border-spacing: 0;
}

hr
{
    display: block;
    height: 1px;
    border: 0;
    border-top: 1px solid #888;
    margin: 1em 0;
    padding: 0;
}

input, select
{
    vertical-align: middle;
}

body
{
    font-family: Calibri, Tahoma, Helvetica, Arial, sans-serif;
    line-height: 115%;
    margin: 1em 1.5em 2em 1.5em;
}

h1, h2, h3, h4, h5, h6 {
    margin-top: 0.66em;
    margin-bottom: 0.33em;
    line-height: 100%;
}
h1, h2, h3, h4 {
    font-family: Cambria, Times, "Times New Roman", serif;
    font-weight: normal;
}
h1 {
    font-size: 2.3em;
}
h2 {
    font-size: 1.8em;
}
h3 {
    font-size: 1.4em;
}
h4 {
    font-size: 1.1em;
}
h5, h6 {
    font-family: Calibri, Tahoma, Helvetica, Arial, sans-serif;
}
h5 {
    font-size: 1.1em;
    font-weight: bold;
}
h6 {
    text-decoration: underline;
    font-weight: normal;
}

hr {
    clear: both;
}

p, ul, ol, pre, figure, table {
    margin-top: 1.00em;
}

a {
    color: #0850ff;
    text-decoration: none;
}
a:visited {
    color: #4060A0;
}
a:hover {
    color: #0080ff;
}
a:active {
    color: #dc143c;
}

h1 a, h1 a:visited,
h2 a, h2 a:visited,
h3 a, h3 a:visited,
h4 a, h4 a:visited,
h5 a, h5 a:visited,
h6 a, h6 a:visited
{
    color: inherit;
}

h1 a:hover, h2 a:hover, h3 a:hover, h4 a:hover, h5 a:hover, h6 a:hover {
    color: #0080ff;
}
h1 a:active, h2 a:active, h3 a:active, h4 a:active, h5 a:active, h6 a:active {
    color: #dc143c;
}


img {
    border: none;
}

ins, mark, del {
    border-radius: 0.25em;
    padding: 0 0.2em;
}

ins
{
    background-color: #9f9;
    border: 1px solid #6b6;
    color: #000;
    text-decoration: none;
}

mark
{
    background-color: #ff9;
    border: 1px solid #bb6;
    color: #000;
    font-style: italic;
}

del
{
    background-color: #f99;
    border: 1px solid #b66;
    color: #000;
    text-decoration: line-through;
}

li ul, li ol {
    margin-top: 0.33em;
    margin-bottom: 0.33em;
}
li {
    margin-left: 2em;
}
ul li {
    list-style: disc outside none;
}
ul li li {
    list-style: circle outside none;
}
ul li li li {
    list-style: square outside none;
}
ol li {
    list-style: decimal outside none;
}
ol li li {
    list-style: lower-latin outside none;
}
ol li li li {
    list-style: lower-roman outside none;
}

li p  {
    margin-top: 0;
}

nav li {
    list-style: none outside none;
    margin-left: 0;
}

q {
    quotes: "\201E" "\201C";
    font-style: italic;
}
q:before {
    content: open-quote;
}
q:after {
    content: close-quote;
}

blockquote {
    border: 1px #ddd solid;
    border-radius: 0.5em;
    margin: 1em 10% 1em 10%;
    padding: 0.5em;
    font-style: italic;
}
blockquote cite {
    display: block;
    text-align: right;
    margin-top: 0.5em;
    font-style: normal;
    font-size: 0.8em;
}
blockquote > p:first-child {
    margin-top: 0;
}

code {
    font-family: Consolas, "Lucida Console", Courier, monospace;
    font-size: 0.8em;
    color: #024;
    background-color: #f6f6f6;
    padding: 0 0.2em;
    border: 1px solid #e4e4e4;
    border-radius: 0.2em;
}
pre code {
    background-color: inherit;
    border: none;
    border-radius: none;
}
pre {
    border: 1px #ddd solid;
    border-radius: 0.5em;
    padding: 0.5em;
    white-space: pre;
    overflow: auto;
}

table {
    border-collapse: collapse;
}
td, th {
    padding: 0.125em 0.5em 0.125em 0.5em;
    border: 1px #ddd solid;
}
thead td, th, tfoot td {
    font-weight: bold;
}

thead, tfoot {
    background-color: #F0F0F0;
}

figure {
    border: 1px #ddd solid;
    border-radius: 0.5em;
    margin-left: 10%;
    margin-right: 10%;
    padding: 0.5em;
    text-align: center;
    overflow: auto;
}
figure figcaption {
    display: block;
    text-align: center;
    font-style: normal;
    font-weight: bold;
    font-size: 0.8em;
    margin-top: 0.5em;
}

figure table {
    width: 100%;
    margin-top: 0;
}

section {
    border-top: 1px #aaa solid;
    margin-top: 2em;
}

footer {
    margin-top: 2em;
    text-align: center;
    color: #888;
}

aside {
    float: right;
    width: 20%;
    border: 1px #ddd solid;
    border-radius: 0.5em;
    margin-left: 1em;
    padding: 0 0.5em 1em 0.5em;
}

body {
    margin: 0;
    padding: 0;
    background-color: #F0F0F0;
}

#frame {
    text-align: left;
    padding: 1em;
    background-color: #FFFFFF;
    -moz-box-shadow:    0 0 12px #808080;
    -chrome-box-shadow: 0 0 12px #808080;
    box-shadow:         0 0 12px #808080;
}

header {
    padding-bottom: 1em;
}

figure {
  margin-left: 0;
  margin-right: 0;
}

/*** NAV ***/

nav ul {
    list-style: none;
    margin-top: 0px;
    padding-top: 0px;
    margin-left: 0px;
    padding-left: 0px;
}

nav .menu-title {
    font-weight: bold;
}

/*** NAV HORIZONTAL ***/

nav.horizontal {
    border-bottom: 1px solid #AAA;
    width: auto;
    float: none;
    padding: 0.25em;
}

nav.horizontal .menu-title, nav.horizontal ul {
    display: inline-block;
}

nav.horizontal .menu-title {
    padding: 0 0.5em 0 0.25em;
}

nav.horizontal li {
    display: inline-block;
    padding: 0 0.5em 0 0.5em;
}

/*** NAV VERTICAL ***/

nav.vertical {
    border-bottom: none;
    width: 10em;
    float: left;
    padding: 1em 1em 1em 0.5em;
}

nav.fixed {
    position: fixed;
}

nav.vertical .menu-title, nav.vertical ul {
    display: block;
}

nav.vertical .menu-title {
    padding: 0 0 0.5em 0;
}

nav.vertical li {
    display: block;
    padding: 0 0 0.25em 0;
}

nav.vertical li li {
    margin-left: 0.75em;
    font-size: small;
    padding: 0;
}

#page {
    margin: 1em 0.5em 1em 11em;
    padding-left: 1em;
}

footer {
    border-top: 1px solid #AAA;
    clear: both;
}

@media screen and (min-width: 960px) and (min-device-width: 960px) {
    #frame {
        width: 920px;
        margin-left: auto;
        margin-right: auto;
    }
}

@media screen and (max-width: 720px), screen and (max-device-height: 720px) and (orientation: landscape) {
    #frame {
        max-width: 720px;
    }

    nav.vertical {
        width: auto;
        float: none;
        margin: 0;
        padding: 0.25em;
    }

    nav.fixed {
        position: relative;
    }

    nav.vertical {
        border-bottom: 1px solid #AAA;
        width: auto;
        float: none;
        padding: 0.25em;
    }

    nav.vertical .menu-title, nav.vertical ul {
        display: inline-block;
    }

    nav.vertical .menu-title {
        padding: 0 0.5em 0 0.25em;
    }

    nav.vertical li {
        display: inline-block;
        padding: 0 0.5em 0 0.5em;
    }

    #page {
        margin: 1em 0.5em;
        padding: 0;
    }
}

@media screen and (max-width: 480px) and (orientation: portrait) {
    #frame {
    max-width: 480px;
        padding: 0.25em;
    }
  header {
        padding-bottom: 0;
    }
    header h1 {
        margin: 0;
        padding-bottom: 0.25em;
    padding-top: 0.25em;
        font-size: 1.8em;
        border-bottom: 1px solid #AAA;
        text-align: center;
    }

    nav.vertical, nav.horizontal {
        border-bottom: 1px solid #AAA;
        width: auto;
        float: none;
        padding: 0.5em 0em;
        text-align: center;
    }

    nav.vertical .menu-title, nav.vertical ul, nav.horizontal .menu-title {
        display: block;
    }

    nav.vertical .menu-title, nav.horizontal .menu-title {
        padding: 0 0 0.5em 0;
    }

    nav.vertical li, nav.horizontal li {
        display: block;
        padding: 0 0 0.25em 0;
    }
}

header h1#title {
    margin-top: 0.25em;
}

header h1#subtitle {
    margin-top: 0.5em;
    font-size: 1.6em;
}

#preface {

}
#preface .item {
    margin: 0.5em 0;
}
#preface .item .label {
    font-weight: bold;
    display: block;
    float: left;
    width: 6em;
    overflow: hidden;
    text-overflow: ellipsis;
}
#preface .item .text {
    display: block;
    margin-left: 7em;
}
#preface .item .affiliation {
    display: block;
    margin-left: 7em;
    clear: both;
    font-style: italic;
    color: #888;
}

    </style>
    </head>
<body>
<div id="frame">
    <header>
        
        <h1 id="title">Forschungsergebnisse</h1>
        <h1 id="subtitle">OLL - Verschriftlichung von Videosequenzen</h1>
    </header>
    <a id="TOC"></a>
    <nav class="vertical fixed">
    	<ul>
<li><a href="#zusammenfassung">Zusammenfassung</a></li>
<li><a href="#zielstellung">Zielstellung</a></li>
<li><a href="#tonspurextraktion">Tonspurextraktion</a></li>
<li><a href="#spracherkennungssysteme">Spracherkennungssysteme</a><ul>
<li><a href="#spracherkennung">Spracherkennung</a></li>
<li><a href="#methodik-und-bausteine">Methodik und Bausteine</a></li>
<li><a href="#systeme">Systeme</a></li>
<li><a href="#informationen-über-erkannte-worte">Informationen über erkannte Worte</a></li>
</ul></li>
<li><a href="#filterung-der-ergebnisse">Filterung der Ergebnisse</a><ul>
<li><a href="#relevante-worte">Relevante Worte</a></li>
<li><a href="#filterkriterien">Filterkriterien</a></li>
</ul></li>
<li><a href="#darstellung-der-ergebnisse">Darstellung der Ergebnisse</a><ul>
<li><a href="#präsentationsformat">Präsentationsformat</a></li>
<li><a href="#wortwolke">Wortwolke</a></li>
<li><a href="#glossar">Glossar</a></li>
<li><a href="#transkript-und-video">Transkript und Video</a></li>
<li><a href="#zuordnung-von-videos-zu-kategorien">Zuordnung von Videos zu Kategorien</a></li>
</ul></li>
<li><a href="#erkenntnisse">Erkenntnisse</a><ul>
<li><a href="#qualitätsfaktoren">Qualitätsfaktoren</a></li>
<li><a href="#kompressionsartefakte">Kompressionsartefakte</a></li>
<li><a href="#sprecherprofile">Sprecherprofile</a></li>
<li><a href="#laufzeitverhalten">Laufzeitverhalten</a></li>
</ul></li>
<li><a href="#fazit-und-ausblick">Fazit und Ausblick</a></li>
<li><a href="#anhang">Anhang</a></li>
<li><a href="#quellen">Quellen</a></li>
</ul>
    </nav>
    <div id="page">
        <div id="preface">
            <div class="item association">
                <span class="label">Projekt</span>
                <span class="text">Online-Lehr- und Lernhilfsmittel (OLL)</span>
            </div><div class="item association">
                <span class="label">Teilprojekt</span>
                <span class="text">Verschriftlichung von Videosequenzen</span>
            </div><div class="item association">
                <span class="label">Förderung</span>
                <span class="text">EFRE MWFK - Antrags-Nr. 80156701</span>
            </div>
            <div class="item creator">
                <span class="label">Projektleiter</span>
                <span class="text">Friedhelm Mündemann                <a href="mailto:muendemann@fh-brandeburg.de">muendemann@fh-brandeburg.de</a>                </span>
                                <span class="affiliation">Fachhochschule Brandenburg, Fachbereich Informatik und Medien</span>
                            </div><div class="item creator">
                <span class="label">Mitarbeiter</span>
                <span class="text">Tobias Kiertscher                <a href="mailto:kiertscher@fh-brandenburg.de">kiertscher@fh-brandenburg.de</a>                </span>
                                <span class="affiliation">Fachhochschule Brandenburg, Fachbereich Informatik und Medien</span>
                            </div>
            <div class="item date">
                <span class="label">Datum</span>
                <span class="text">31.01.2014</span>
            </div>
            <div class="item version">
                <span class="label">Version</span>
                <span class="text">1</span>
            </div>
        </div>
    	<h1 id="zusammenfassung"><a href="#zusammenfassung">Zusammenfassung</a></h1>
<p>Im Rahmen des Projekte <em>Online-Lehr- und Lernhilfsmittel (OLL)</em> an der <a href="http://www.fh-brandenburg.de/" title="Fachhochschule Brandenburg">Fachhochschule Brandenburg</a> wurde im Teilprojekt <em>Verschriftlichung von Videosequenzen</em> ein automatisierter Prozess implementiert und erforscht, mit dem eine Verschriftlichung von Videos möglich ist. Neben einem Transkript werden Worthäufigkeitslisten für jedes Video erzeugt. Die Videos können anschließend automatisch einer benutzerdefinierten Anzahl von Kategorien zugeordnet werden. Dadurch ist z.B. die automatische Verschlagwortung und thematische Einordnung von Videomaterial mit unbekanntem gesprochenem Inhalt möglich. Der Prozess wird durch eine grafische Benutzeroberfläche gesteuert. Projekte mit Videos, Kategorien und Steuerparametern können gespeichert und wiederverwendet werden. Die Ergebnisse werden u.a. als <abbr title="Hyper Text Markup Language">HTML</abbr>5-Webseite und als maschinenlesbares <abbr title="eXtensible Markup Language">XML</abbr> ausgeben. Die Webseite bietet eine ansprechende visuelle Aufbereitung der erkannten Worte und Texte und eine übersichtliche Präsentation der Übereinstimmung zwischen den Videos und der Kategorien.</p>
<p>Das Projekt wurde gefördert mit Mitteln des Europäischen Fonds für Regionalentwicklung (EFRE) in der Bewirtschaftung des MWFK Programms “e-learning und e-knowledge”. Das Projekt wird unter der Antragsnummer 80156701 geführt.</p>
<h1 id="zielstellung"><a href="#zielstellung">Zielstellung</a></h1>
<p>Das Ziel des Teilprojektes ist, das Testen und lauffähig machen eines automatisierten Prozesses zur Extraktion von Tonspuren aus einem Video, Verschriftlichung der Tonspur und Erstellen einer Worthäufigkeitsliste.</p>
<p>Dazu soll zunächst eine Software gefunden und getestet werden, mit der die Tonspur aus einem Video extrahiert und in einem geeigneten Format gespeichert werden kann.</p>
<p>Des Weiteren soll eine Anzahl von unterschiedlicher Diktatsoftware untersucht werden. Zu prüfen ist, ob sich die Software für die Einbindung in einen automatisierten Prozess eignet und wie die Unterstützung für eine Vielzahl unbekannter Sprecher ist.</p>
<p>Die erkannten Worte sollen nach relevanten Worten gefiltert werden. Die Filterung soll durch Parameter gesteuert werden können. Es soll eine Worthäufigkeitsliste und daraus ein Glossar erzeugt werden. Es soll untersucht werden, wie die Worthäufigkeitsliste innerhalb des automatisierten Prozesses durch eine Wortwolke visualisiert werden kann.</p>
<p>Darüber hinaus ist zu prüfen, ob es möglich ist, Links zu erzeugen, die von erkannten Worten aus dem Volltext oder aus dem Glossar zu den Positionen ihres Auftretens im Video führen.</p>
<p>Als weiteres Ziel des Projektes sollen die Videos benutzerdefinierten Kategorien zugeordnet werden. Eine Kategorie soll dabei durch eine Wortliste definiert werden. Im Ergebnis sollen sowohl für ein Video alle zugeordnete Kategorien angezeigt werden, als auch umgekehrt für eine Kategorie alle zugeordneten Videos.</p>
<p>Die Ergebnisse des Prozesses sollen visuell aufbereitet und zusätzlich in einem Format für die weitere maschinelle Verarbeitung gespeichert werden.</p>
<p>Um das Projekt sinnvoll abzugrenzen werden alle Untersuchungen und Entwicklungen auf die deutsche Sprache eingeschränkt.</p>
<p>Die Software die im Rahmen dieses Projektes entsteht, soll als quelloffenes Projekt veröffentlicht werden.</p>
<h1 id="tonspurextraktion"><a href="#tonspurextraktion">Tonspurextraktion</a></h1>
<p>Für die Verschriftlichung von Videomaterial ist lediglich die Analyse der Tonspur erforderlich. Aus diesem Grund muss diese vor der weiteren Verarbeitung zunächst aus jedem Video extrahiert werden.</p>
<p>Digitales Videomaterial wird in sog. Container-Formaten gespeichert. Beispiele sind <abbr title="Audio Video Interleave (Microsoft)">AVI</abbr>, <abbr title="QuickTime Container-Format (Apple)">MOV</abbr>, <abbr title="MPEG-4 Container-Format">MP4</abbr>, <abbr title="Matroska Container-Format">MKV</abbr>. Ein Container-Format enthält i.d.R. Strukturen für Meta-Daten (Urheber, Sprache, etc.) und unterstützt ein oder mehrere Bild- und ein oder mehrere Tonspuren.</p>
<p>Das Verbinden von den digitalen Datenströmen für das Bild und den Ton eines Videos in einem Container wird als <em>muxen</em> bezeichnet, das Trennen von Bild und Tonspur <em>demuxen</em>. Die Bild- und Tonspuren können, um Speicherplatz und Übertragungsbandbreite zu sparen, in komprimierter Form gespeichert werden. Dabei kommen sowohl für Bild- als auch Tonspuren eine Vielzahl von unterschiedlichen verlustfreien und verlustbehafteten Kompressionsverfahren zum Einsatz. Beispiele sind M-<abbr title="Joint Photographic Experts Group">JPEG</abbr>, <abbr title="Moving Picture Experts Group">MPEG</abbr>-1, <abbr title="Moving Picture Experts Group">MPEG</abbr>-2, <abbr title="Moving Picture Experts Group">MPEG</abbr>-4/<abbr title="Advanced Video Coding (MPEG)">AVC</abbr> (H.264) für Bildspuren und <abbr title="Free Lossless Audio Codec">FLAC</abbr>, <abbr title="MPEG-1 Audio Layer 3">MP3</abbr>, <abbr title="Advanced Audio Coding (MPEG)">AAC</abbr> (<abbr title="Moving Picture Experts Group">MPEG</abbr>-4-Audio) für Tonspuren. In einigen Fällen ist es auch sinnvoll Bild- und Tondaten unkomprimiert zu speichern. Jedoch ist auch in diesem Fall ein Container-Format für das muxen erforderlich.</p>
<p>Ist die Tonspur in einem Video mehrkanalig, z.B. im Stereo-Format, sollte die Tonspur vor der Spracherkennung in eine Mono-Tonspur umgewandelt werden.</p>
<p>Für das Speichern einer Tonspur als Audio-Datei sind entsprechend der verschiedenen Kompressionsverfahren auch unterschiedliche Dateiformate üblich. Soll die Tonspur mit einem anderen Kompressionsformat gespeichert werden, als sie in einem Video-Container vorgefunden wurde, muss die Tonspur neu kodiert werden.</p>
<p>Für die Extraktion einer Tonspur aus einem digital gespeicherten Video sind folglich drei Schritte notwendig:</p>
<ol type="1">
<li>Tonspur aus Container-Datei extrahieren ( demuxen )</li>
<li>Tonspur falls erforderlich neu kodieren</li>
<li>Tonspur in Audio-Datei speichern</li>
</ol>
<p>Damit eine Software zur Verschriftlichung von Videomaterial eine möglichst große Anzahl der gängigen Container- und Kompressionsformate unterstützt, ist ein Programm erforderlich, welches eine große Anzahl von Container- und Kompressionsformaten lesend unterstützt und Tonspuren in einem für das Spracherkennungssystem verständlichen Format speichern kann.</p>
<p>Das Projekt <a href="http://www.ffmpeg.org" title="FFmpeg">FFmpeg</a> stellt eine quelloffene und Plattform-unabhängige Lösung für das Aufzeichnen, Konvertieren und Streamen von Audio- und Videomaterial unter der <abbr title="Lesser GNU Public License">LGPL</abbr> 2.1 zur Verfügung. FFmpeg enthält die Codec-Bibliothek libavcodec, welche durch die OpenSource-Community gepflegt wird und Bestandteil vieler freier Medienplayer ist. Teil des Projektes ist das Befehlszeilenwerkzeug <code>ffmpeg</code> welches eine automatische Erkennung des Formates einer Videodatei und auch alle drei oben genannten Schritte zur Extraktion der Tonspur beherrscht.</p>
<p>Für das Projekt OLL wird FFmpeg für die Extraktion der Tonspur aus Videos verwendet.</p>
<h1 id="spracherkennungssysteme"><a href="#spracherkennungssysteme">Spracherkennungssysteme</a></h1>
<p>Im folgenden Abschnitt werden zunächst einige Grundlagen zur Spracherkennung und Verschriftlichung erläutert und anschließend ein existierendes Spracherkennungssystem für das Projekt OLL ausgewählt.</p>
<h2 id="spracherkennung"><a href="#spracherkennung">Spracherkennung</a></h2>
<p>Hinter dem Begriff <em>Spracherkennung</em> verbergen sich zwei unterschiedliche Zielstellungen. Zum einen gibt es die Spracherkennung auf der Basis einer Grammatik für die Steuerung durch Sprache. Diese Form wird z.B. in Telefonsystemen eingesetzt und zeichnet sich dadurch aus, dass für eine Vielzahl unbekannter Sprecher ein relativ kleiner Wortschatz erkannt und in Steuerbefehle umgesetzt werden soll. Und zum anderen gibt es die Spracherkennung für die Verschriftlichung, auch als Diktiersystem bezeichnet. Diese Form zeichnet sich dadurch aus, dass für einen bekannten Sprecher ein großer Wortschatz erkannt und in einen sauberen Text umgesetzt werden soll.</p>
<p>Für die Verschriftlichung von unbekanntem Videomaterial besteht die besondere Herausforderung darin, dass für eine große Anzahl unbekannter Sprecher ein großer Wortschatz erkannt werden soll. Bereits 1989 wurde dieses Anwendungsfeld mit begrenztem Erfolg erforscht (vergl. <span class="citation" data-cites="Levinson+1989">(Levinson et al. 1989)</span>). Eine gute Zusammenfassung zu dem aktuellen Forschungsstand findet sich in <span class="citation" data-cites="SaonChien2012">(G. Saon and Chien 2012)</span>.</p>
<h2 id="methodik-und-bausteine"><a href="#methodik-und-bausteine">Methodik und Bausteine</a></h2>
<p>Die aktuellen Spracherkennungssysteme arbeiten überwiegend nach dem folgenden Prinzip. Die Tonspur wird anhand der auftretenden Pausen in Abschnitte unterteilt (Phrasen). Jede Phrase wird nach Lauten durchsucht (Phoneme). Die Laute werden mit Hilfe eines akustischen Modells in ein Lautalphabet, wie das <em>Internationale Phonetische Alphabet</em> (<abbr title="International Phonetic Alphabet">IPA</abbr>) übersetzt. Dabei werden die Klangeigenschaften (z.B. Frequenzspektrum) als Merkmal verwendet. Die Lautfolgen werden nun benutzt, um Wörter aus einem vorgegebenen Wortschatz auszuwählen. Der Wortschatz kann in Form einer kleinen Grammatik (Befehlssteuerung) oder einem großen Lexikon (Verschriftlichung) definiert sein. Ein Wortschatz wird häufig in der <em>Pronunciation Lexicon Specification</em> (<abbr title="Pronunciation Lexicon Specification (W3C)">PLS</abbr>) Notation gespeichert. Um die Laufzeit der Suche in einem großen Wortschatz zu verbessern, wird ein <em>Hidden-Markov-Model</em> (<abbr title="Hidden-Markov-Model">HMM</abbr>) verwendet. Das <abbr title="Hidden-Markov-Model">HMM</abbr> gibt die Wahrscheinlichkeit an, mit der ein Wort nach einem bereits erkannten Wort auftritt, dadurch muss für jedes potentielle Wort nur noch ein kleiner Teil des Wortschatzes durchsucht werden.</p>
<p>Dabei treten die folgenden Bausteine auf:</p>
<ul>
<li>Akustisches Modell</li>
<li>Lautalphabet</li>
<li>Wortschatz</li>
<li>Hidden-Markov-Model / N-Gramme</li>
</ul>
<h3 id="akustisches-modell"><a href="#akustisches-modell">Akustisches Modell</a></h3>
<p>Das akustische Modell bildet die Aussprache eines Sprechers und die Klangeigenschaften der Aufnahmetechnik auf ein Lautalphabet ab. Für Spracherkennungssysteme die Verschriftlichung unterstützen, wird für jeden potentiellen Sprecher ein eigenes akustisches Modell als Teil eines Sprecherprofils angelegt. Das akustische Modell wird trainiert, indem der Sprecher einen Text vorliest, dessen Lautfolge dem System bekannt ist.</p>
<p>Seit den 90er Jahren wurden adaptive akustische Modelle erforscht. Das Ziel dieser Modelle ist es, die Erkennungsleistung für einen Sprecher über eine längere Benutzungsphase kontinuierlich zu steigern (vergl. <span class="citation" data-cites="Thelen1996">(Thelen 1996)</span>). Des Weiteren wurden Ansätze erforscht, das akustische Modell derart zu normalisieren, dass es in Teilen sprecherunabhängig wird und in Folge keine sprecherspezifische Trainingsphase mehr erforderlich macht (vergl. <span class="citation" data-cites="Wegmann1996">(Wegmann et al. 1996)</span> und <span class="citation" data-cites="Lee1998">(Lee and Rose 1998)</span>).</p>
<h3 id="lautalphabet"><a href="#lautalphabet">Lautalphabet</a></h3>
<p>Ein Lautalphabet ist ein Alphabet welches alle Laute einer vokalen Kommunikationsform enthält. Im Gegensatz zum Alphabet einer Schriftsprache beschreiben die Elemente eines Lautalphabetes nicht die Zeichen aus denen Worte in Schriftform gebildet werden, sondern die kleinsten identifizierbaren Klangelemente die bei der Aussprache eines Wortes verwendet werden.</p>
<p>Ein verbreitetes Lautalphabet ist das Internationale Phonetische Alphabet. Bei einigen Spracherkennungssystemen kommen aber auch herstellerspezifische Lautalphabete zum Einsatz.</p>
<h3 id="wortschatz"><a href="#wortschatz">Wortschatz</a></h3>
<p>Die Grammatik oder das Lexikon eines Spracherkennungssystems bildet die Lautfolgen einer vokalen Sprache auf die Zeichenfolgen einer Schriftsprache ab. Dabei werden einem Wort der Schriftsprache ein oder mehrere mögliche Lautfolgen zugeordnet. Umgekehrt kann eine Lautfolge aber auch mehreren Worten zugeordnet sein. Der Wortschatz für ein Spracherkennungssystem richtet sich stark nach der Zielstellung der Spracherkennung. So werden für Befehlssteuerungen nur kleine Wortschätze verwendet (Größenordnung 10 - 100 Befehle), während für die Verschriftlichung alle Worte der verwendeten natürlichen Sprache enthalten sein müssen. Der Umfang eines Lexikons für die Verschriftlichung variiert in Abhängigkeit der zu erwartenden Fachdomäne, z.B. Nachrichten, Medizin, Jura (Größenordnung 5.000 - 500.000 Worte).</p>
<p>Je kleiner der Wortschatz ist, desto stärker unterscheiden sich die Lautfolgen der Wörter voneinander. Infolgedessen ist die Erkennung der Worte sicherer. Bei einem großen Wortschatz treten zusätzlich zu mehr Ähnlichkeiten auch zunehmend Homophone (unterschiedliche Wort mit gleicher Aussprache) auf.</p>
<p>Neben der Zuordnung eines Wortes zu ein oder mehreren Lautfolgen gibt ein Wortschatz auch oft die Rolle eines Wortes in der Sprache an (z.B. Substantiv, Adjektiv, Verb). Durch diese Angabe kann Wissen über die Grammatik einer natürlichen Sprache verwendet werden, um aus mehreren möglichen erkannten Worten das wahrscheinlichste auszuwählen.</p>
<p>Zur Speicherung eines Wortschatzes hat sich inzwischen weitgehend die von der <abbr title="World Wide Web Consortium">W3C</abbr> herausgegebene Empfehlung <em>Pronunciation Lexicon Specification</em> (<abbr title="Pronunciation Lexicon Specification (W3C)">PLS</abbr>) durchgesetzt, welche auf der <em>eXtensible Markup Language</em> (<abbr title="eXtensible Markup Language">XML</abbr>) aufsetzt.</p>
<p>Jüngere Ansätze haben die automatische Erkennung von zusammengesetzten Worten, die nicht im Wortschatz enthalten sind, zum Gegenstand (vergl. <span class="citation" data-cites="Saon1999">(George Saon and Padmanabhan 1999)</span>). Des Weiteren werden auch Verfahren untersucht, die den thematischen Kontext einer Phrase bei der Auswahl der erkannten Worte berücksichtigen (vergl. <span class="citation" data-cites="Bellegarda2000">(Bellegarda 2000)</span> und <span class="citation" data-cites="Wang2007">(Wang, McCallum, and Wei 2007)</span>).</p>
<h3 id="hidden-markov-model-n-gramme"><a href="#hidden-markov-model-n-gramme">Hidden-Markov-Model / N-Gramme</a></h3>
<p>Das <abbr title="Hidden-Markov-Model">HMM</abbr> für ein Lexikon gibt die Wahrscheinlichkeit für das Auftreten eines Worts an. Dabei wird die Wahrscheinlichkeit in Bezug zu den vorher erkannten Wörtern gesetzt. In diesem Kontext spricht man auch von N-Grammen. Das Hidden-Markov-Model für einen Wortschatz wird i.d.R. aus bereits existierenden Texten gewonnen und erfordert einen rechenintensiven Trainingsprozess.</p>
<h2 id="systeme"><a href="#systeme">Systeme</a></h2>
<p>Bei der Recherche im Rahmen des Projektes OLL wurden die folgenden Spracherkennungssysteme gefunden. Dabei wurden nur Systeme berücksichtigt, welche einen Diktiermodus für die Verschriftlichung besitzen.</p>
<ul>
<li><a href="http://www.linguatec.com" title="Voice Pro von Linguatec">Voice Pro</a> (VP) von Linguatec (ehemals Via Voice von <abbr title="International Business Machines Coporation">IBM</abbr>)</li>
<li><a href="http://www.nuance.com" title="Dragon Natural Speaking von Nuance">Dragon Natural Speaking</a> (DNS) von Nuance Communications</li>
<li><a href="http://msdn.microsoft.com/de-de/library/ee125077.aspx" title="Microsoft Speech API">MS Speech <abbr title="Application Programming Interface">API</abbr></a> (S<abbr title="Application Programming Interface">API</abbr>) von Microsoft</li>
<li><a href="http://cmusphinx.sourceforge.net" title="SPHINX">Sphinx</a> von <abbr title="Carnegie Mellon University">CMU</abbr></li>
<li><a href="http://julius.sourceforge.jp/en_index.php" title="Julius">Julius</a> von Nagoya Institute of Technology (ehemals von Kyoto University)</li>
</ul>
<h3 id="bewertungskriterien"><a href="#bewertungskriterien">Bewertungskriterien</a></h3>
<p>Für die Auswahl eines Systems für die Verschriftlichung von Tonspuren im Projekt OLL werden die folgenden Auswahlkriterien verwendet.</p>
<ul>
<li><strong>Einarbeitungsaufwand gering</strong><br /> Der Aufwand für die Einrichtung eines funktionstüchtigen Spracherkennungssystems muss von einem Entwickler im Rahmen der Projektlaufzeit bewältigt werden können. Dabei steht nicht die gesamte Projektlaufzeit für die Einarbeitung in das Spracherkennungssystem zur Verfügung, vielmehr muss sie mit den anderen Teilaufgaben (Tonspurextraktion, Glossar-Generierung, Ergebnisdarstellung, Dokumentation) geteilt werden.</li>
<li><strong>Bausteine verfügbar</strong><br /> Damit eine erfolgreiche Inbetriebnahme des Spracherkennungssystems im Rahmen des Projektes möglich ist, müssen alle Bausteine (akustisches Modell, Lexikon, <abbr title="Hidden-Markov-Model">HMM</abbr>) für die deutsche Sprache verfügbar sein oder mit vertretbarem Aufwand erstellt werden können.</li>
<li><strong>Technische Voraussetzungen erfüllt</strong><br /> Das Spracherkennungssystem muss auf einem handelsüblichen PC und einem an der Hochschule verfügbaren Betriebssystem lauffähig sein. Dazu gehören z.B. eine aktuelle Linux-Distribution wie SuSE, Fedora oder Ubuntu und Microsoft Windows in den Versionen XP, Vista, 7 und 8. Des Weiteren muss das Spracherkennungssystem durch eine Programmierschnittstelle (<abbr title="Application Programming Interface">API</abbr> / <abbr title="Software Development Kit">SDK</abbr>) angesteuert werden können.</li>
<li><strong>Kosten gering</strong> (einmalig, laufend)<br /> Durch das Spracherkennungssystem dürfen keine laufenden Kosten entstehen. Einmalige Lizenzkosten dürfen die Größenordnung von 1000,00 EURO nicht überschreiten.</li>
<li><strong>Transparent</strong><br /> Das System sollte im besten Fall im Quelltext vorliegen und ausreichend dokumentiert sein.</li>
<li><strong>Flexibel</strong><br /> Das System sollte konfigurierbar sein, um eine Optimierung für den konkreten Anwendungsfall zu erleichtern. Dazu sollte es möglich sein, das akustische Modell, den Wortschatz und das <abbr title="Hidden-Markov-Model">HMM</abbr> auszutauschen oder anzupassen.</li>
</ul>
<p>Auf das Auswahlkriterium Erkennungsgenauigkeit muss verzichtet werden, da die Systeme in dieser Hinsicht nicht vergleichbar sind. Es existieren keine normierten Testfälle und dazugehörige Testergebnisse für alle Systeme. Die Erarbeitung entsprechender Testfälle und Durchführung der Tests würde den Rahmen des Projektes übersteigen.</p>
<h3 id="auswahl"><a href="#auswahl">Auswahl</a></h3>
<p>Jedes System wird unter jedem Auswahlkriterium mit 0 bis 2 bewertet. Erfüllt ein System ein Kriterium nicht, erhält es 0. Erfüllt ein System ein Kriterium ausreichend, erhält es 1. Erfüllt ein System ein Kriterium in besonderem Maße, erhält es 2.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Kriterium</th>
<th style="text-align: center;">VP</th>
<th style="text-align: center;">DNS</th>
<th style="text-align: center;">S<abbr title="Application Programming Interface">API</abbr></th>
<th style="text-align: center;">Sphinx</th>
<th style="text-align: center;">Julius</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Einarbeitungsaufwand gering</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Bausteine verfügbar</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Technische Voraussetzungen erfüllt</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Kosten gering</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Transparent</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: left;">Flexibel</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
</tbody>
</table>
<p>Die kommerziellen Produkte Voice Pro und Dragon Natural Speaking kommen nicht in Frage, da eine Lizenz, für eine Version die eine stapelweise Verschriftlichung von Tonspuren unterstützt, mehrere tausend Euro kostet. Des Weiteren könnte ein Projektergebnis auf Basis dieser Produkte nicht ohne weiteres von Dritten nachvollzogen werden. Interessant ist Voice Pro von Linguatech dennoch, da es, laut Hersteller-Website, die als bisher sehr schwierig geltende sprecherunabhängige Spracherkennung von allgemeiner Sprache im Diktat beherrschen soll.</p>
<p>Die quelloffenen Projekte Sphinx und Julius erfordern einen hohen Einarbeitungsaufwand, da für ein lauffähiges System umfangreicher Konfigurations- und teilweise Programmieraufwand anfällt. Der Autor konnte kein vollständig konfiguriertes Paket für die Verschriftlichung von deutscher Sprache auf Basis dieser Projekte finden. Eine Einarbeitung in diese Projekte lohnt sich jedoch bei ausreichender Projektlaufzeit, da sie weitgehend konfigurierbar und damit flexibler sind als die Alternativen.</p>
<p>Die Speech <abbr title="Application Programming Interface">API</abbr> von Microsoft (Version 5) ist in jedem Microsoft Windows Betriebssystem ab Windows XP enthalten. Die unterstützte Sprache hängt von der Sprache des Systems ab. Ein englisches Windows unterstützt ein Diktat in englischer Sprache, ein deutsches Windows unterstützt ein Diktat in deutscher Sprache. Das akustische Modell muss sprecherabhängig trainiert werden. Das trainierte Sprecherprofil kann exportiert und auf einer anderen Installation wieder importiert werden. Der Wortschatz ist allgemein gehalten und ausreichend groß. Die Speech <abbr title="Application Programming Interface">API</abbr> kann sowohl über C++ (<abbr title="Microsoft Foundation Classes">MFC</abbr>) als auch über C# (.NET) angesprochen werden.</p>
<p>Für das Projekt OLL wird Microsoft Speech <abbr title="Application Programming Interface">API</abbr> (Version 5.4 als Teil von Windows 7) als Spracherkennungssystem gewählt.</p>
<h2 id="informationen-über-erkannte-worte"><a href="#informationen-über-erkannte-worte">Informationen über erkannte Worte</a></h2>
<p>Die Microsoft Speech <abbr title="Application Programming Interface">API</abbr> gibt bei einem Spracherkennungsprozess nicht nur die erkannten Worte aus, sondern darüber hinaus die folgenden Informationen.</p>
<ul>
<li>Anfang und Länge einer erkannten Phrase in Sekunden</li>
<li>Erkennungssicherheit einer Phrase als Wert zwischen 0 und 1</li>
<li>Zeichenkette eines Worte</li>
<li>Lautfolge eines Wortes in der <abbr title="International Phonetic Alphabet">IPA</abbr>-Notation</li>
<li>Erkennungssicherheit eines Wortes als Wert zwischen 0 und 1</li>
<li>Alternative Wortfolgen für eine Phrase</li>
</ul>
<h1 id="filterung-der-ergebnisse"><a href="#filterung-der-ergebnisse">Filterung der Ergebnisse</a></h1>
<p>Anschließend an die Spracherkennung soll die Möglichkeit untersucht werden, relevante Worte aus der Menge der erkannten Worte heraus zu filtern. Die Herausforderung besteht dabei aus zwei Teilen. Zunächst muss eine zielführende Definition für den Begriff <em>relevant</em> gefunden werden. Anschließend müssen Filterkriterien gefunden werden, die mit angemessenem Aufwand relevante Worte von nicht relevanten Worten unterscheidet. Des Weiteren müssen auch Filterkriterien gefunden werden, welche die Erkennungssicherheit der Worte berücksichtigt.</p>
<h2 id="relevante-worte"><a href="#relevante-worte">Relevante Worte</a></h2>
<p>Für die Abgrenzung relevanter Worte von nicht relevanten Worten, muss die Zielstellung des Projektes betrachtet werden. Wesentliche Ergebnisse des Prozesses sollen Worthäufigkeitslisten und eine Zuordnung zu Kategorien sein. Da nicht vorgegeben ist, welche Art von Wörtern die Worthäufigkeitslisten enthalten sollen, hilft diese Anforderung nicht bei der Abgrenzung relevanter Wörter. Die Zuordnung zu Kategorien kann jedoch eine Hilfe sein, um eine Abgrenzung zu finden.</p>
<p>In Folge soll der Begriff <em>relevant</em> dahingehend konkretisiert werden, dass ein Wort dann relevant ist, wenn es dabei hilft ein Video zu einer Kategorie zu zuordnen. Damit keine Missverständnisse über den Begriff <em>Kategorie</em> entstehen, muss an dieser Stelle kurz dem weiter unten folgendem Abschnitt <em>Zuordnung von Videos zu Kategorien</em> vorgegriffen werden: Eine Kategorie ist ein Begriff, der durch eine Worthäufigkeitsliste mit anderen Worten in Beziehung gesetzt wird. Der Grad, mit dem ein Text einer Kategorie zugeordnet wird, entspricht der Ähnlichkeit zwischen der Worthäufigkeitsliste des Textes und der Worthäufigkeitsliste der Kategorie.</p>
<p>Die Zuordnung zwischen einem Video und einer Kategorie soll im Idealfall dem Inhalt nach erfolgen und nicht z.B. nach der Anzahl gleicher Artikel oder Hilfsverben. Also muss für die Zuordnung eine Wortart gefunden werden, die einerseits einfach zu identifizieren ist, und andererseits den größten Teil des Inhalts transportiert.</p>
<h2 id="filterkriterien"><a href="#filterkriterien">Filterkriterien</a></h2>
<p>Die Filterkriterien bilden sich zum einen aus der Wortrelevanz und zum anderen aus der Erkennungssicherheit.</p>
<h3 id="relevanz"><a href="#relevanz">Relevanz</a></h3>
<p>In der deutschen Sprache erscheinen die Substantive und die Eigennamen als zwei viel versprechende Wortarten, um ein Relevanz-Kriterium zu bilden. Denn einerseits legt die deutsche Sprache einen Großteil der thematischen Bedeutung in diese beiden Wortarten und andererseits können Substantive und Eigennamen durch die Großschreibung leicht von anderen Wortarten unterschieden werden. Dabei sollte jedoch nach Möglichkeit die Grundform eines Wortes verwendet werden und nicht die grammatikalisch transformierte Form, da in der deutschen Sprache alle Wortarten am Anfang eines Satzes großgeschrieben werden.</p>
<p>Um Substantive bei der Zuordnung auszuschließen, die keine besondere thematische Bedeutung tragen, sondern in nahezu allen Texten vorkommen, erscheint es sinnvoll eine Liste mit allgemein häufig auftretenden Worten als Blacklist zu verwenden.</p>
<h3 id="erkennungssicherheit"><a href="#erkennungssicherheit">Erkennungssicherheit</a></h3>
<p>Da das ausgewählte Spracherkennungssystem die Erkennungssicherheit für jedes Wort ausgibt, bietet sich diese Größe ebenfalls als Filterkriterium an. Denn Worte die nur mit einer sehr geringen Sicherheit, und damit im Umkehrschluss häufig falsch erkannt wurden, können die Zuordnung zu einer Kategorie störend beeinflussen. Da die Erkennungssicherheit eines erkannten Wortes als Zahl zwischen 0 und 1 gegeben ist, kann ein Schwellwert von z.B. 0,7 als Kriterium verwendet werden.</p>
<p>Zu der Ausgabe des Spracherkennungssystems gehören auch Zahlen, Satzzeichen, Sonderzeichen und Abkürzungen in der Form <code>E. U.</code>. Deshalb wurden Filterkriterien implementiert die Worte mit nicht alphabetischen Zeichen und Worte mit weniger als drei Zeichen ausschließen.</p>
<h1 id="darstellung-der-ergebnisse"><a href="#darstellung-der-ergebnisse">Darstellung der Ergebnisse</a></h1>
<p>In diesem Abschnitt wird beschrieben, welche Erkenntnisse über die Darstellung der Spracherkennungsergebnisse gewonnen wurden. Des Weiteren werden einige Entwurfsentscheidungen für das Projektergebnis dokumentiert.</p>
<h2 id="präsentationsformat"><a href="#präsentationsformat">Präsentationsformat</a></h2>
<p>Es wurde untersucht, mit welchem Ausgabeformat die Ergebnisse der Spracherkennung am besten dargestellt werden können. Dabei wurden die folgenden Formate in Erwägung gezogen.</p>
<ul>
<li>Reintext</li>
<li><abbr title="eXtensible Markup Language">XML</abbr></li>
<li>einfaches <abbr title="Hyper Text Markup Language">HTML</abbr></li>
<li><abbr title="Hyper Text Markup Language">HTML</abbr>5 mit JavaScript</li>
<li><abbr title="Portable Document Format">PDF</abbr></li>
</ul>
<h3 id="bewertungskriterien-1"><a href="#bewertungskriterien-1">Bewertungskriterien</a></h3>
<ul>
<li><strong>Visuelle Ausdruckskraft</strong><br /> Bewertet wie vielfältig das Layout und die Formatierung gestaltet werden können ist.</li>
<li><strong>Interaktivität</strong><br /> Bewertet wie stark Interaktionen mit dem Benutzer zur Navigation und Anpassung der Darstellung möglich sind.</li>
<li><strong>Multimedia</strong><br /> Bewertet wie gut multimediale Inhalte eingebettet werden können.</li>
<li><strong>Portabilität</strong><br /> Bewertet wie leicht die Ausgabe auf andere Geräte übertragen werden kann.</li>
<li><strong>Maschinenlesbarkeit</strong><br /> Bewertet wie gut die gespeicherten Informationen für eine weitere Verarbeitung durch Scripts genutzt werden können.</li>
<li><strong>Aufwand bei Erstellung</strong><br /> Bewertet den Aufwand der für Umsetzung notwendig ist. (Eine hohe Zahl bedeutet geringen Aufwand, eine niedrige Zahl hohen Aufwand.)</li>
</ul>
<h3 id="auswahl-1"><a href="#auswahl-1">Auswahl</a></h3>
<p>Die Formate werden unter jedem Kriterium bewertet. Die Bewertung erfolgt zwischen 0 und 2.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Kriterium</th>
<th style="text-align: center;">Text</th>
<th style="text-align: center;"><abbr title="eXtensible Markup Language">XML</abbr></th>
<th style="text-align: center;"><abbr title="Hyper Text Markup Language">HTML</abbr></th>
<th style="text-align: center;"><abbr title="Hyper Text Markup Language">HTML</abbr>5/JS</th>
<th style="text-align: center;"><abbr title="Portable Document Format">PDF</abbr></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Visuelle Ausdruckskraft</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td style="text-align: left;">Interaktivität</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Multimedia</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">Portabilität</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Maschinenlesbarkeit</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">Aufwand bei Erstellung</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Die Darstellung in Reintext bietet sich für die Ausgabe eines Transkripts für ein einzelnes Video an, ist einfach zu erzeugen, kann jedoch keine zusätzlichen Informationen (wie Erkennungssicherheit) aufnehmen ohne die Darstellung zu stören und bietet keine Multimediaunterstützung (z.B. Einbettung des Quellvideos).</p>
<p>Das <abbr title="eXtensible Markup Language">XML</abbr>-Format eignet sich sehr gut, um alle während der Spracherkennung gewonnenen Informationen strukturiert zu speichern. <abbr title="eXtensible Markup Language">XML</abbr>-Dateien lassen sich in nahezu allen üblichen Programmiersprachen einfach weiterverarbeiten. <abbr title="eXtensible Markup Language">XML</abbr> kann durch ein CSS in begrenztem Maße formatiert und potentiell durch ein XSLT in <abbr title="Hyper Text Markup Language">HTML</abbr> umgewandelt werden.</p>
<p>Einfaches <abbr title="Hyper Text Markup Language">HTML</abbr> bietet reichhaltige Möglichkeiten zur Formatierung der Ergebnisse. Es können leicht mehrere Seiten zur Darstellung verwendet werden, die miteinander verknüpft sind. Es eignet sich auch gut für die Verbreitung der Ergebnisse über das Internet. <abbr title="Hyper Text Markup Language">HTML</abbr> wird auf einer Vielzahl von Geräten unterstützt. Es können ohne Probleme Bilder (z.B. Wortwolken) eingebunden werden, es besitzt jedoch keine gute Unterstützung für Video oder Audio. Der Aufwand für die Erzeugung von <abbr title="Hyper Text Markup Language">HTML</abbr> hält sich in Grenzen.</p>
<p>Die Version 5 von <abbr title="Hyper Text Markup Language">HTML</abbr> im Zusammenspiel mit JavaScript erbt die positiven Eigenschaften von <abbr title="Hyper Text Markup Language">HTML</abbr> und erweitert diese um eine gute Unterstützung für weitgehende Interaktion, Video und Audio. Die einzige Einschränkung ist, dass es einen möglichst aktuellen Browser voraussetzt. Bei der Produktion von interaktiven <abbr title="Hyper Text Markup Language">HTML</abbr>5-Seiten entsteht durch die zusätzliche JavaScript-Programmierung ein erhöhter Aufwand.</p>
<p>Das <em>Portable Document Format</em> (<abbr title="Portable Document Format">PDF</abbr>) von Adobe hat sich in vielen Bereichen für die Weitergabe von Dokumenten durchgesetzt. Seine größte Stärke ist dabei, dass Formatierung und Layout auf jedem unterstützten Gerät weitgehend identisch reproduziert wird. Nachteil dieses Formates ist die mangelnde Unterstützung für Video und Interaktivität. Auch der Aufwand für die Erzeugung eines <abbr title="Portable Document Format">PDF</abbr> ist relativ hoch.</p>
<p>Die Ergebnisse des Prozesses im Projekt OLL werden in <abbr title="Hyper Text Markup Language">HTML</abbr>5 mit JavaScript ausgegeben. Zusätzlich werden die erkannten Worte der Videos als Transkript in Reintext ausgegeben und detaillierte Informationen über die erkannten Worte, einschließlich Erkennungssicherheiten und einer Worthäufigkeitsliste als <abbr title="eXtensible Markup Language">XML</abbr>-Datei gespeichert.</p>
<h2 id="wortwolke"><a href="#wortwolke">Wortwolke</a></h2>
<p>Es wurde untersucht, ob die Anwendung <a href="http://www.mosaizer.com/Wordaizer/" title="wordizer">wordizer</a> von APP Helmond durch Scripts angesteuert und so, zur Erzeugung von Wortwolken, in den automatisierten Prozess eingebunden werden kann.</p>
<p>Um die Wortwolke in eine interaktive Oberfläche einbinden zu können wurde eine Anforderung ergänzt, die in der Zielstellung nicht aufgeführt ist: Die Anwendung zur Erzeugung der Wortwolke muss neben der bildlichen Darstellung der Wortwolke auch die Positionen und Flächen der Worte in der Wortwolke als Datensatz ausgeben. Nur dadurch wird es möglich, die Worte der Wortwolke als aktive Links zu gestalten und damit zur Navigation zu den entsprechenden Details zu nutzen.</p>
<p>Das Ergebnis der Untersuchung ergab, dass wordizer weder mit angemessenem Aufwand durch Scripts angesteuert werden kann, noch die Möglichkeit bietet, die Wortpositionen und -flächen auszugeben. In Folge wurde nach weiteren Anwendungen recherchiert, die Wortwolken erzeugen können. Keine der gefundenen Anwendungen bot die Möglichkeit die Wortpositionen und -flächen auszugeben.</p>
<p>Aus diesem Grund wurde eine neue Programmbibliothek (mastersign.cloud) zur Erzeugung von Wortwolken implementiert, welche die gestellten Anforderungen erfüllt. Als Grundlage wurde der von Jonathan Feinberg – Entwickler von <a href="http://www.wordle.net" title="Wordle">Wordle</a> – auf <a href="http://stackoverflow.com/questions/342687" title="Wordle Algorithmus">stackoverflow</a> veröffentlichte Algorithmus verwendet. Als technische Plattform diente die <a href="http://java.oracle.com" title="Java Plattform von Oracle">Java</a> Laufzeitumgebung und die Programmiersprache <a href="http://clojure.org" title="Clojure Programming Language">Clojure</a>.</p>
<p>Die erzeugten Wortwolken können zwei Größen visualisieren:</p>
<ul>
<li>die Häufigkeit eines Wortes wird durch die Schriftgröße dargestellt</li>
<li>die Sicherheit mit der ein Wort durchschnittlich erkannt wurde wird durch die Schriftfarbe dargestellt</li>
</ul>
<p>Für die möglichen Ausrichtungen der Worte wurde horizontal, vertikal links gedreht und vertikal rechts gedreht gewählt. Bei der Verwendung dieser Ausrichtungen kann eine gute Lesbarkeit und gleichzeitig eine hohe Ausnutzung des verfügbaren Platzes erreicht werden (siehe <a href="#img:cloud-unsorted">Abbildung 1</a>).</p>
<p><a name="img:cloud-unsorted"></a></p>
<figure>
<img src="images/cloud_unsorted.png" alt="Abbildung 1: Eine mit mastersign.cloud erzeugte Wortwolke" /><figcaption>Abbildung 1: Eine mit mastersign.cloud erzeugte Wortwolke</figcaption>
</figure>
<p>Als Weiterentwicklung der bekannten Wortwolkenformen wurde der Algorithmus dahingehend modifiziert, dass Worte wenn möglich alphabetisch sortiert platziert werden (siehe <a href="#img:cloud-sorted">Abbildung 2</a>). Worte mit den Anfangsbuchstaben A bis N werden tendenziell in der oberen Hälfte der Wortwolke von links nach rechts positioniert. Worte mit den Anfangsbuchstaben O-Z werden tendenziell in der unteren Hälfte von links nach rechts positioniert. Diese Form der Sortierung ermöglicht möglicherweise intuitiv das schnelle Auffinden eines bekannten Wortes in der Wortwolke.</p>
<p><a name="img:cloud-sorted"></a></p>
<figure>
<img src="images/cloud_sorted.png" alt="Abbildung 2: Eine sortierte Wortwolke" /><figcaption>Abbildung 2: Eine sortierte Wortwolke</figcaption>
</figure>
<h2 id="glossar"><a href="#glossar">Glossar</a></h2>
<p>Gemäß der Zielstellung wurde untersucht, in welcher Form die erkannten Worte durch ein Glossar zugänglich gemacht werden können.</p>
<p>In der <abbr title="Hyper Text Markup Language">HTML</abbr>5-Ausgabe des Prozesses werden für jedes Video ein individueller Glossar und für alle Videos ein gemeinsamer Glossar erzeugt (siehe <a href="#img:glossary">Abbildung 3</a>). Der Glossar erlaubt die Navigation zwischen den Buchstaben des Alphabets.</p>
<p><a name="img:glossary"></a></p>
<figure>
<img src="images/glossary.png" alt="Abbildung 3: Ein Glossar mit alphabetischer Navigation" /><figcaption>Abbildung 3: Ein Glossar mit alphabetischer Navigation</figcaption>
</figure>
<h2 id="transkript-und-video"><a href="#transkript-und-video">Transkript und Video</a></h2>
<p>Des Weiteren wurde untersucht wie das Transkript eines Videos dargestellt werden kann und ob eine Navigation von erkannten Worten zu den Positionen im Video möglich ist, an denen das Wort auftritt.</p>
<p>Durch die Unterstützung von <abbr title="Hyper Text Markup Language">HTML</abbr>5 für eingebettetes Video ist es möglich das Video gemeinsam mit dem Transkript in einer Seite darzustellen. Da der Video-Player in einem <abbr title="Hyper Text Markup Language">HTML</abbr>5-fähigen Browser eine Steuerung durch JavaScript unterstützt ist es auch möglich automatisch an bestimmte Positionen in einem Video zu springen und das Video ab dieser Position abzuspielen.</p>
<p>Das Transkript wird in einem Bereich unter dem Video angezeigt (siehe <a href="#img:video-transcript">Abbildung 4</a>). Es besteht aus einer Liste mit Phrasen. Vor jeder Phrase steht die Position der Phrase im Video. Durch einen Klick auf die Uhrzeit springt der Video-Player an die entsprechende Position und startet die Wiedergabe. Die Worte der Phrasen werden farblich abgestuft dargestellt. Worte mit kräftiger Farbe wurden mit hoher Sicherheit erkannt, Worte mit schwacher Farbe wurden mit geringer Sicherheit erkannt. Ein Klick auf ein Wort leitet zu einer Detailseite für das jeweilige Wort (siehe <a href="#img:video-word">Abbildung 5</a>).</p>
<p><a name="img:video-transcript"></a></p>
<figure>
<img src="images/video_transcript.png" alt="Abbildung 4: Der Video-Player und das zum Video gehörende Transkript" /><figcaption>Abbildung 4: Der Video-Player und das zum Video gehörende Transkript</figcaption>
</figure>
<p><a name="img:video-word"></a></p>
<figure>
<img src="images/video_word.png" alt="Abbildung 5: Der Video-Player mit den Details eines Wortes einschließlich der Phrasen in denen das Wort vorkommt" /><figcaption>Abbildung 5: Der Video-Player mit den Details eines Wortes einschließlich der Phrasen in denen das Wort vorkommt</figcaption>
</figure>
<h2 id="zuordnung-von-videos-zu-kategorien"><a href="#zuordnung-von-videos-zu-kategorien">Zuordnung von Videos zu Kategorien</a></h2>
<p>Eine Kategorie wird durch eine Worthäufigkeitsliste definiert. Die Worthäufigkeitsliste für eine Kategorie kann aus einem oder mehreren Texten, die aneinander gehängt werden, gewonnen werden. In dem in diesem Projekt implementierten Prozess können Reintexte, <abbr title="Hyper Text Markup Language">HTML</abbr>-Seiten und mit besonderer Berücksichtigung <a href="http://www.mediawiki.org" title="Media Wiki">MediaWiki</a>-Seiten verwendet werden. Die Texte werden durch einen URL referenziert und können somit sowohl lokal, als auch im Internet gespeichert sein.</p>
<p>Die besondere Berücksichtigung von MediaWiki-Seiten wurde implementiert, um die Definition einer Kategorie durch <a href="http://de.wikipedia.org" title="Deutsche Wikipedia">Wikipedia</a>-Seiten zu erleichtern. Durch das Vorwissen über den Aufbau von MediaWiki-Seiten ist es möglich Wiki-spezifische Worte und <abbr title="Hyper Text Markup Language">HTML</abbr>-Element vor der weiteren Verarbeitung heraus zu filtern.</p>
<p>Da der Zweck der Worthäufigkeitsliste einer Kategorie darin besteht mit der Worthäufigkeitsliste eines Videos verglichen zu werden, können die Texte einer Kategorie mit den gleichen Kriterien gefiltert werden wie die erkannten Worte eines Videos. Mit dem Unterschied dass die Worte in den Texten einer Kategorie keine Erkennungssicherheit besitzen und deshalb eine Erkennungssicherheit von 1 angenommen werden muss.</p>
<p>Der eigentliche Zuordnungsprozess erfolgt durch die Multiplikation der Häufigkeit eines Wortes aus dem Video mit der Häufigkeit des gleichen Wortes aus der Kategorie und die Summierung der multiplizierten Worthäufigkeiten. Kommt ein Wort nur in einer der beiden Worthäufigkeitslisten vor, wird es nicht berücksichtigt.</p>
<p>Da die Anzahl der erkannten Worte in einem Video und die Anzahl der Worte in den Texten einer Kategorie variieren kann, müssen die Worthäufigkeiten vor der Multiplikation normiert werden. Das geschieht mittels der Division durch die größte Worthäufigkeit der jeweiligen Worthäufigkeitsliste. Im Ergebnis kann eine multiplizierte Worthäufigkeit den Wert 1 nicht überschreiten. Und die Summe der multiplizierten Worthäufigkeiten kann die Anzahl der Worte nicht überschreiten, die sowohl im Video als auch in der Kategorie vorkommen. Dieser Wert wird in den Prozessergebnissen als <em>Übereinstimmung</em> bezeichnet.</p>
<p>Werden <em>m</em> Videos <em>n</em> Kategorien zugeordnet, wird die Übereinstimmung <em>m x n</em> mal berechnet. Diese Übereinstimmungen werden in der Prozessausgabe als Matrix dargestellt (siehe <a href="#img:matrix">Abbildung 6</a>).</p>
<p><a name="img:matrix"></a></p>
<figure>
<img src="images/matrix.png" alt="Abbildung 6: Eine Übereinstimmungsmatrix für einige Videos und Kategorien" /><figcaption>Abbildung 6: Eine Übereinstimmungsmatrix für einige Videos und Kategorien</figcaption>
</figure>
<p>Die Hintergrundfarben der Zellen in der Übereinstimmungsmatrix geben die Übereinstimmung relativ zur besten Übereinstimmung für das jeweilige Video an. Die Prozentzahl in den Zellen gibt die Übereinstimmung relativ zur besten Übereinstimmung für alle Videos an.</p>
<h1 id="erkenntnisse"><a href="#erkenntnisse">Erkenntnisse</a></h1>
<p>Aus den theoretischen Betrachtungen und den praktischen Erfahrungen im Umgang mit dem implementierten Prozess konnten die folgenden Erkenntnisse gewonnen werden.</p>
<h2 id="qualitätsfaktoren"><a href="#qualitätsfaktoren">Qualitätsfaktoren</a></h2>
<p>Die Erkennungsleistung des Spracherkennungssystems hängt von einer Vielzahl von Faktoren ab, die bei der Einschätzung der Prozessergebnisse berücksichtigt werden müssen.</p>
<h3 id="technische-faktoren"><a href="#technische-faktoren">Technische Faktoren</a></h3>
<ul>
<li><strong>Mikrofon</strong><br /> Die Eigenschaften des Mikrofons (Dynamik, Aufnahmecharakteristik, Empfindlichkeit für Erschütterungen und Pop-Geräusche) beeinflussen maßgeblich die Qualität des aufgenommenen Audiosignals und damit die Erkennungsleistung.<br /> Die besten Ergebnisse können mit hochwertigen Headset- oder Lavelier-Mikrofonen erreicht werden.</li>
<li><strong>Umgebungsgeräusche</strong><br /> Werden neben dem Sprecher Umgebungsgeräusche aufgezeichnet, können diese zu falsch erkannten Wörtern führen. Ein Spracherkennungssystem besitzt in der Regel eine Filterstufe, die versucht Geräusche von vokalen Lauten zu unterscheiden.</li>
<li><strong>Abstand des Sprechers zum Mikrofon</strong><br /> Der Abstand zwischen dem Mund des Sprechers und dem Mikrofon ist je nach Mikrofon mehr oder weniger wichtig für ein ausreichend kräftiges Audio-Signal. Ist der Abstand groß sind Umgebungsgeräusche relativ zum Sprecher lauter. Ist der Abstand zu gering treten häufiger Pop-Geräusche oder sogar Geräusche durch die Berührung zwischen Mund und Mikrofon auf. Ein Spracherkennungssystem versucht in der Regel den charakteristischen Klang einer Sprecher-Mikrofon-Kombination in der Trainingsphase des akustischen Modells zu berücksichtigen bzw. kennen zu lernen.</li>
<li><strong>Kompressionsartefakte</strong> Die meisten Tonspuren in Videos werden mit einem verlustbehafteten Kompressionsverfahren kodiert. Diese Art der Kodierung erzeugt eine mehr oder weniger starke Verzerrung des Tonsignals, die bei stärkerer Kompression auch zu hörbaren Artefakten führt. Die gängigen Kompressionsverfahren versuchen dabei, gemäß eines psychoakustischen Modells die Frequenzanteile, die für das menschliche Hören wichtig sind, so wenig wie möglich zu verzerren.<br /> Dieser Faktor wird im Abschnitt <em>Kompressionsartefakte</em> noch genauer untersucht.</li>
</ul>
<h3 id="menschliche-faktoren"><a href="#menschliche-faktoren">Menschliche Faktoren</a></h3>
<ul>
<li><strong>Dialekt</strong>, <strong>Akzent</strong><br /> Der deutsche Wortschatz eines Spracherkennungssystems berücksichtigt i.d.R. nur die Standardsprache mit ihrer relativ gut definierten Aussprache und den dadurch gegebenen Lautfolgen. Benutzt der Sprecher einen Dialekt oder spricht er mit Akzent, passt seine Aussprache weniger zu den Lautfolgen die im Wortschatz hinterlegt sind. Daraus resultiert eine zum Teil drastisch verschlechterte Erkennungsleistung.</li>
<li><strong>Deutlichkeit der Aussprache</strong><br /> Spricht der Sprecher undeutlich, nimmt die Erkennungsleistung des Systems ab.</li>
<li><strong>Geschwindigkeit der Aussprache</strong><br /> Spricht ein Sprecher sehr schnell oder sehr langsam, kann das Spracherkennungssystem die Grenzen der einzelnen Laute und Worte nicht mehr sauber erkennen und die Erkennungsleistung nimmt ab.</li>
</ul>
<h3 id="systemfaktoren"><a href="#systemfaktoren">Systemfaktoren</a></h3>
<ul>
<li><strong>Training des akustischen Modells</strong> (Sprecherprofile)<br /> Ein für dieses Projekt besonders wichtiger Faktor ist das Training des akustischen Modells. Dieser Faktor wird im Abschnitt <em>Sprecherprofile</em> noch genauer untersucht.</li>
<li><strong>Abdeckung des Wortschatzes</strong><br /> Wird der Wortschatz des Sprechers nur unzureichend vom Wortschatz des Systems abgedeckt, können nur die allgemeineren Worte aus der deutschen Kernsprache erkannt werden. Das führt zum einen zu einer allgemein schlechteren Erkennungsleistung. Zum anderen führt es aber auch dazu, dass insbesondere jene Worte denen die größte inhaltliche Bedeutung zukommt und damit die größte Relevanz besitzen, nicht für die Zuordnung eines Video zu einer Kategorie zur Verfügung stehen.<br /></li>
<li><strong>Anzahl der Worte in einer Aufnahme</strong><br /> Die Anzahl der Worte in der Aufnahme, verändert nicht die Erkennungsleistung des Spracherkennungssystems, da die Microsoft Speech <abbr title="Application Programming Interface">API</abbr> jede Phrase individuell analysiert. Dabei kommen keine adaptiven Verfahren zum Einsatz, die die Erkennungsleistung in Abhängigkeit der bereits erkannten vorhergehenden Worte beeinflusst. Bei anderen Spracherkennungssystem kann das anders sein. Die Qualität der Zuordnung eines Videos zu einer Kategorie wird jedoch sehr wohl durch die Anzahl der Worte in einer Aufnahme beeinflusst. Denn die Worthäufigkeiten und die daraus errechnete Übereinstimmung sind umso aussagekräftiger, je mehr Worte dabei berücksichtigt wurden.</li>
<li><strong>Anzahl der Worte einer Kategorie</strong><br /> Für die Anzahl der Worte in einer Kategorie gibt es vermutlich ein jedoch in diesem Projekt nicht näher untersuchtes Optimum. Denn wenn eine Kategorie durch zu wenige Worte beschrieben wird, steigt die Wahrscheinlichkeit, dass in einem Video andere Worte verwendet wurden, um über das gleiche Thema zu sprechen. Wenn jedoch eine Kategorie durch zu viele Worte beschrieben wird, sinkt die Trennschärfe zwischen mehreren Kategorien und die Aussagekraft der Zuordnung sinkt.</li>
</ul>
<h2 id="kompressionsartefakte"><a href="#kompressionsartefakte">Kompressionsartefakte</a></h2>
<p>Um die Abhängigkeit der Erkennungsleistung von der Kompressionsrate der Tonspur zu untersuchen, wurde das folgende Experiment durchgeführt. Es wurde ein Profil trainiert und durch den gleichen Sprecher, mit dem gleichen Mikrofon und bei vergleichbaren Umgebungsgeräuschen ein bekannter Text vorgelesen und aufgenommen. Als Mikrofon kam ein Headset der Firma Logitech mit integrierter USB-Soundkarte zum Einsatz. Die Speicherung der Aufnahme erfolgte im nicht komprimierenden <abbr title="Pulse Code Modulation">PCM</abbr>-Wave-Format bei einer Auflösung von 16 Bit und einer Abtastrate von 44,1 kHz.</p>
<p>Diese Datei wurde mit Hilfe von FFmpeg zunächst in verschiedene Auflösungen (16 und 8 Bit) und Abtastraten umgerechnet (44.100, 32.000, 22.050, 16.000, 11.025, 8.000 Hz). Des Weiteren wurde die Originaldatei mit Hilfe des <abbr title="MPEG-1 Audio Layer 3">MP3</abbr>-Lame-Encoders (Version 3.99.5) und des experimentellen <abbr title="Advanced Audio Coding (MPEG)">AAC</abbr>-Encoders von FFmpeg (Version N-58699-ge3d7a39 vom 01.12.2013) für verschiedene konstante Bit-Raten komprimiert (128, 96, 64, 48, 32, 24, 16, 8 kBit/s).</p>
<p>Die Originaldatei und die verschiedenartig kodierten abgeleiteten Varianten wurden anschließend mit der Microsoft Speech <abbr title="Application Programming Interface">API</abbr> verarbeitet. Die Erkennungsergebnisse geben über die Abhängigkeit der Erkennungsleistung von der Qualität des Ausgangsmaterials Auskunft.</p>
<h3 id="auflösung-und-abtastrate"><a href="#auflösung-und-abtastrate">Auflösung und Abtastrate</a></h3>
<p>Wie in <a href="#img:compression-16">Abbildung 7</a> und <a href="#img:compression-8">Abbildung 8</a> zu erkennen ist, ist die Erkennungsleistung der Bei 16- und 8-Bit-Auflösung sehr ähnlich, was darauf schließen lässt, dass die Spracherkennung für 8-Bit-Auflösung optimiert ist und von einer höheren Auflösung nicht profitieren kann.</p>
<p>Des Weiteren ist die Erkennungsleistung bei 44.100, 32.000, 22.050 und 16.000 Hz nahezu identisch mit einer geringfügig schwächeren Leistung bei der Original-Abtastrate von 44.100 Hz. Bei reduzierten Abtastraten von 11.025 und 8.000 Hz bricht die Erkennungsleistung deutlich ein. Wodurch die leicht erhöhte Erkennungsleitung zustande kommt, wenn Ausgangsmaterial von 44.100 Hz auf 16.000 - 32.000 Hz herunter gerechnet wird konnte im Projekt nicht eindeutig geklärt werden. Es lässt sich vermuten, dass durch die Reduzierung der Bandbreite ein Glättungseffekt erzielt wird, der den Rauschanteil im Signal senkt.</p>
<p>Im Hinblick auf eine effiziente Speichernutzung für Audiodaten lässt sich sagen, dass für die Microsoft Speech <abbr title="Application Programming Interface">API</abbr> eine 8-Bit-Kodierung und eine Abtastrate von 16.000 Hz ausreicht, da das Spracherkennungssystem von höherer Auflösung oder Abtastrate nicht profitieren kann.</p>
<p><a name="img:compression-16"></a></p>
<figure>
<img src="images/compression_16bit.png" alt="Abbildung 7: Die durchschnittliche Erkennungssicherheit für Phrasen und Worte bei 16-Bit-Auflösung und unterschiedlichen Abtastraten" /><figcaption>Abbildung 7: Die durchschnittliche Erkennungssicherheit für Phrasen und Worte bei 16-Bit-Auflösung und unterschiedlichen Abtastraten</figcaption>
</figure>
<p><a name="img:compression-8"></a></p>
<figure>
<img src="images/compression_8bit.png" alt="Abbildung 8: Die durchschnittliche Erkennungssicherheit für Phrasen und Worte bei 8-Bit-Auflösung und unterschiedlichen Abtastraten" /><figcaption>Abbildung 8: Die durchschnittliche Erkennungssicherheit für Phrasen und Worte bei 8-Bit-Auflösung und unterschiedlichen Abtastraten</figcaption>
</figure>
<h3 id="verlustbehaftete-kompression"><a href="#verlustbehaftete-kompression">Verlustbehaftete Kompression</a></h3>
<p>Besonders interessant für das Projekt ist die Auswirkung einer verlustbehafteten Kompression des Audiosignals auf die Spracherkennungsleistung, da eine Tonspur in den gängigen Videoformaten in dieser Weise gespeichert wird. Hier wurde die Auswirkung der <abbr title="MPEG-1 Audio Layer 3">MP3</abbr>- und der <abbr title="Advanced Audio Coding (MPEG)">AAC</abbr>-Kompression untersucht. In <a href="#img:compression-mp3">Abbildung 9</a> ist zu erkennen, dass die Erkennungsleistung für <abbr title="MPEG-1 Audio Layer 3">MP3</abbr> von 128 bis 48 kBit/s relativ stabil bleibt und unterhalb von 48 kBit/s deutlich abfällt. Die Erkennungsleistung von <abbr title="Advanced Audio Coding (MPEG)">AAC</abbr> (vergl. <a href="#img:compression-aac">Abbildung 10</a>) verhält sich ebenfalls bis 48 kBit/s stabil und liegt etwas über der von <abbr title="MPEG-1 Audio Layer 3">MP3</abbr>. Unterhalb von 48 kBit/s fällt die Erkennungsleistung noch schneller ab als bei <abbr title="MPEG-1 Audio Layer 3">MP3</abbr>.</p>
<p>Die schlechte Erkennungsleistung bei der <abbr title="Advanced Audio Coding (MPEG)">AAC</abbr>-Kompression mit niedrigen Bitraten ist eventuell auf den experimentellen <abbr title="Advanced Audio Coding (MPEG)">AAC</abbr>-Codec von FFmpeg zurück zu führen. FFmpeg unterstützt in der verwendeten Version mehrere Encoder-Bibliotheken für <abbr title="Advanced Audio Coding (MPEG)">AAC</abbr>: <code>libvo_aacenc</code>, <code>aac</code>, <code>libfaac</code> und <code>libfdk_aac</code>. In der Dokumentation werden diese Encoder in der hier aufgeführten Reihenfolge der Kodierungsqualität nach geordnet. Wobei <code>libvo_aacenc</code> die schlechteste und <code>libfdk_aac</code> die beste Qualität produziert. Da die Binärpakete für FFmpeg aus lizenztechnischen Gründen nur <code>libvo_aacenc</code> und <code>aac</code> enthalten, wurde für diese Experimente die noch als experimentell eingestufte eingebaute Bibliothek <code>aac</code> verwendet. In der Dokumentation wird jedoch auf Grund des deutlichen Qualitätsunterschieds die Verwendung der Bibliothek <code>libfdk_aac</code> empfohlen, welche am Frauenhofer Institut entwickelt wird.</p>
<p><a name="img:compression-mp3"></a></p>
<figure>
<img src="images/compression_mp3.png" alt="Abbildung 9: Die durchschnittliche Erkennungssicherheit für Phrasen und Worte bei 16-Bit und 44.100 Hz mit MP3-Kompression und unterschiedlichen Bit-Raten" /><figcaption>Abbildung 9: Die durchschnittliche Erkennungssicherheit für Phrasen und Worte bei 16-Bit und 44.100 Hz mit <abbr title="MPEG-1 Audio Layer 3">MP3</abbr>-Kompression und unterschiedlichen Bit-Raten</figcaption>
</figure>
<p><a name="img:compression-aac"></a></p>
<figure>
<img src="images/compression_aac.png" alt="Abbildung 10: Die durchschnittliche Erkennungssicherheit für Phrasen und Worte bei 16-Bit und 44.100 Hz mit AAC-Kompression und unterschiedlichen Bit-Raten" /><figcaption>Abbildung 10: Die durchschnittliche Erkennungssicherheit für Phrasen und Worte bei 16-Bit und 44.100 Hz mit <abbr title="Advanced Audio Coding (MPEG)">AAC</abbr>-Kompression und unterschiedlichen Bit-Raten</figcaption>
</figure>
<p>Zu beachten ist, dass die Bitraten für eine Mono-Tonspur angegeben sind.</p>
<h2 id="sprecherprofile"><a href="#sprecherprofile">Sprecherprofile</a></h2>
<p>Die Microsoft Speech <abbr title="Application Programming Interface">API</abbr> arbeitet mit einem statischen akustischen Modell als Teil des Sprecherprofils. Das bedeutet, dass das Modell einmal trainiert wird und anschließend nur durch Benutzerinteraktion (Angabe von Korrekturen im Diktat) weiter trainiert werden kann. Wurde das akustische Modell von dem gleichen Sprecher mit dem gleichen Mikrofon bei wenigen Umgebungsgeräuschen trainiert wie bei der Tonaufnahme für die Verschriftlichung, sind die Ergebnisse auch als Transkript brauchbar.</p>
<p>Weicht jedoch die Aussprache des Sprechers bei der Tonaufnahme für die Verschriftlichung von der des Sprechers, welcher das akustische Modell trainiert hat, ab, sinkt die Erkennungsleistung des Systems deutlich.</p>
<p>Da in diesem Projekt die Microsoft Speech <abbr title="Application Programming Interface">API</abbr> verwendet wird, und auch keine interaktiven Korrekturen während der Verschriftlichung möglich sind, steht nur ein statisches akustisches Modell für die Erkennung zur Verfügung. Aus diesem Grund wurde der Prozess derart implementiert, dass mehrere Sprecherprofile installiert werden können. Aus diesen wählt dass System für jedes Video jenes Profil aus, das für eine benutzerdefinierte Zeitspanne aus dem Anfang des Video die beste Erkennungsleistung erbringt.</p>
<p>Um die Erkennungsleistung der Microsoft Speech <abbr title="Application Programming Interface">API</abbr> bei der Verwendung von Sprecherprofilen zu testen, die von einem anderen Sprecher trainiert wurden als dem, der die zu erkennende Tonspur gesprochen hat, wurde das folgende Experiment durchgeführt.</p>
<p>Es wurden zwei Videos (1 und 2) mit einer aufgezeichneten Lehrveranstaltung mit jeweils drei Profilen (A, B, und C) verarbeitet, die von unterschiedlichen Sprechern trainiert wurden. Es kamen folglich 5 Sprecher zum Einsatz. Zwei für die Videos und zwei weitere für die Profile.</p>
<p><a name="img:pt-video-1"></a></p>
<figure>
<img src="images/profile-test-video-1.png" alt="Abbildung 11: Die Erkennungssicherheiten von Phrasen und Worten für Video 1 mit Profil A, B, und C" /><figcaption>Abbildung 11: Die Erkennungssicherheiten von Phrasen und Worten für Video 1 mit Profil A, B, und C</figcaption>
</figure>
<p><a name="img:pt-video-2"></a></p>
<figure>
<img src="images/profile-test-video-2.png" alt="Abbildung 12: Die Erkennungssicherheiten von Phrasen und Worten für Video 2 mit Profil A, B, und C" /><figcaption>Abbildung 12: Die Erkennungssicherheiten von Phrasen und Worten für Video 2 mit Profil A, B, und C</figcaption>
</figure>
<p>Wie in <a href="#img:pt-video-1">Abbildung 11</a> und <a href="#img:pt-video-2">Abbildung 12</a> erkennbar ist, passt Profil B sowohl am besten zu Video 1 als auch zu Video 2. Zu dem Profil B ist der Unterschied nicht so groß wie zu dem Profil C, welches mit beiden Videos zu sehr schlechter Erkennungsleistung führt. Interessant ist die Erkennungssicherheit von Phrasen im Video 1. Die für das Profil A sehr wechselhaft ausfällt, wie an Minimum und Maximum abzulesen ist, für Profil B jedoch deutlich stabiler ist.</p>
<h2 id="laufzeitverhalten"><a href="#laufzeitverhalten">Laufzeitverhalten</a></h2>
<p>Die folgenden Größen sind für das Laufzeitverhalten des in diesem Projekt implementierten Prozesses ausschlaggebend:</p>
<p><strong>Eingabedaten</strong></p>
<ul>
<li>Anzahl der Videos</li>
<li>Länge aller Videos</li>
<li>Anzahl der Kategorien</li>
<li>Anzahl aller Worte in den Kategorien</li>
</ul>
<p><strong>System</strong></p>
<ul>
<li>Anzahl der installierten Sprecherprofile</li>
<li>Anzahl der Prozessorkerne</li>
</ul>
<p>Die folgenden Vorgänge benötigen die meiste Zeit:</p>
<ul>
<li>Extraktion der Tonspur (parallel)</li>
<li>Ermitteln des optimalen Sprecherprofils für jede Tonspur (zu jedem Zeitpunkt immer nur ein Sprecherprofil, aber mehrere Tonspuren parallel)</li>
<li>Spracherkennung für die kompletten Tonspuren (zu jedem Zeitpunkt immer nur ein Sprecherprofil, aber mehrere Tonspuren parallel)</li>
<li>Erzeugen der Wortwolken (parallel)</li>
</ul>
<p>Daraus ergibt sich, dass die Laufzeit in der Größenordnung mit erhöhter Anzahl von Videos und erhöhter Anzahl von alternativen Sprecherprofilen im Quadrat steigt.</p>
<p>Sollte es hypothetisch erforderlich sein Tonspuren in anderen Sprachen zu verarbeiten, gilt die Einschränkung, dass für jede Sprache eine eigene Windows Installation in der jeweiligen Sprache erforderlich ist. Dieser Fall wird aber von dem in diesem Projekt implementierten Prozess nicht abgedeckt. <a href="http://www.ffmpeg.org" title="FFmpeg">ffmpeg</a></p>
<h1 id="fazit-und-ausblick"><a href="#fazit-und-ausblick">Fazit und Ausblick</a></h1>
<p>In diesem Projekt wurde die Software FFmpeg für die Extraktion einer Tonspur aus einem Video gefunden und getestet. Diese Software wird über die Befehlszeile gesteuert und kann somit leicht in einen automatisierten Prozess eingebunden werden. FFmpeg unterstützt eine Vielzahl der gängigen Video- und Audio-Formate sowohl als Eingabe als auch als Ausgabe.</p>
<p>Es wurden verschiedene Spracherkennungssystem anhand der folgenden Kriterien verglichen: geringer Einarbeitungsaufwand, Verfügbarkeit aller Bausteine für die Spracherkennung, Erfüllung aller technischen Voraussetzungen, geringe Kosten, Transparenz und Flexibilität.</p>
<p>Entsprechend den Anforderungen und Rahmenbedingungen dieses Projektes wurde die Microsoft Speech <abbr title="Application Programming Interface">API</abbr> (Teil des Microsoft Windows 7 Betriebssystems) als Spracherkennungssystem gewählt. Es lässt sich über eine gut dokumentierte Schnittstelle mit eine Programmiersprache ansteuern, und es gibt eine Reihe von wichtigen Details über die erkannten Worte und Phrasen aus (z.B. Position, Erkennungssicherheit). Es unterstützt die Spracherkennung nur mit Hilfe von trainierten Profilen. Wenn jedoch eine Anzahl von unterschiedlichen Profilen für die Erkennung verwendet wird und die besten Ergebnisse ausgewählt werden, ist die Erkennungsleistung für eine thematische Zuordnung eines Videos ausreichend. Ein ausreichend sauberer Volltext kann jedoch bei Videomaterial mit unbekannten Sprechern nicht erzielt werden. Es bleibt zu prüfen, ob eines der anderen kommerziellen Systeme (z.B. Voice Pro von Linguatec) eine bessere Erkennungsleistung mit unbekannten Sprechern erzielt.</p>
<p>Es wurden verschiedene Filter implementiert, um die erkannten Worte zu filtern. Dabei werden sowohl die Erkennungssicherheit berücksichtigt, als auch lexikalische Eigenschaften wie Wortlänge und Großschreibung. Darüber hinaus ist die Verwendung einer Blacklist möglich. Die Blacklist ist zurzeit auf eine Liste mit den häufigsten kleingeschriebenen Worten der deutschen Sprache festgelegt. In Zukunft könnte die Auswahl und oder Erzeugung einer variablen Blacklist implementiert werden. Die einzelnen Filter können bei Bedarf parametrisiert und wahlweise aktiviert oder deaktiviert werden.</p>
<p>Für die Ausgabe der Ergebnisse wurden die Formate <abbr title="Hyper Text Markup Language">HTML</abbr>5, <abbr title="eXtensible Markup Language">XML</abbr> und Reintext gewählt. Dadurch ist sowohl eine visuell ansprechende Darstellung der Ergebnisse auf einer Vielzahl von Geräten (Desktop, Tablet, Smartphone) möglich, als auch die weitere maschinelle Verarbeitung der Ergebnisse.</p>
<p>Zur Visualisierung der Worthäufigkeiten in Form einer Wortwolke wurde eine neue Programmbibliothek implementiert, welche die Positionen und Flächen der Worte in der Wolke ausgibt und somit die Verlinkung der Worte in der Wolke mit dem Glossar und dem Transkript ermöglicht.</p>
<p>Da die Microsoft Speech <abbr title="Application Programming Interface">API</abbr> zu jeder erkannten Phrase die Position in der Tonspur angibt und <abbr title="Hyper Text Markup Language">HTML</abbr>5 einen Video-Player unterstützt, war es möglich die erkannten Worte mit der Abspielposition des Videos zu verknüpfen. Es kann aus dem Glossar und aus einer Wortwolke zu den Positionen im Video navigiert werden, an denen das jeweilige Wort erkannt wurde.</p>
<p>Für die Zuordnung der Videos zu einer Anzahl von benutzerdefinierten Kategorien wurde eine Kennzahl definiert, mit der die Übereinstimmung zwischen einem Video und einer Kategorie berechnet werden kann. Dazu wird sowohl für das Video als auch für die Kategorie eine Worthäufigkeitsliste erzeugt. Eine Kategorie kann durch eine Menge von Quelltexten beschrieben werden. Als Quelltexte können sowohl lokale Textdateien als auch gewöhnliche Webseiten und insbesondere Wikipedia-Seiten verwendet werden.</p>
<p>Für die Steuerung des automatisierten Prozesses wurde eine grafische Benutzeroberfläche implementiert, welche die Verwaltung der Videos, der Kategorien und der Prozessparameter ermöglicht. Während der Ausführung des Prozesses, die bei einer größeren Anzahl von Videos und Sprecherprofilen mehrere Stunden in Anspruch nehmen kann, werden die einzelnen Prozessschritte und deren Fortschritt übersichtlich dargestellt. Des Weiteren werden alle Schritte in einer Protokolldatei dokumentiert. Es gibt die Möglichkeit Zwischenergebnisse des Prozesses aufzuheben und somit eine wiederholte Ausführung mit veränderter Parametrisierung wesentlich zu Beschleunigen.</p>
<p>Die in diesem Projekt entstandene Software wurde unter dem Namen <em>MediaCategorizer</em> auf der Plattform <a href="http://mastersign.github.io/mediacategorizer" title="MediaCategorizer auf github.com">github</a> unter der <a href="http://opensource.org/licenses/MIT">MIT-Lizenz</a> veröffentlicht.</p>
<h1 id="anhang"><a href="#anhang">Anhang</a></h1>
<p>Die folgenden Dokumente gehören ebenfalls zum Projektergebnis.</p>
<ul>
<li><a href="Benutzerhandbuch.html">MediaCategorizer - Benutzerhandbuch</a></li>
<li><a href="Sprecherprofilverwaltung.html">MediaCategorizer - Sprecherprofile</a></li>
<li><a href="Systemarchitektur.html">MediaCategorizer - Systemarchitektur</a></li>
<li><a href="Webseitenstruktur.html">MediaCategorizer - Struktur der Ergebniswebseite</a></li>
<li><a href="intermediate-data-structures.html">MediaCategorizer - Intermediate Data Structures</a> (englisch)</li>
</ul>
<h1 id="quellen"><a href="#quellen">Quellen</a></h1>
<p>Die folgenden Quellen wurden verwendet.</p>
<div class="references">
<p>Bellegarda, J.R. 2000. “Exploiting Latent Semantic Information in Statistical Language Modeling.” <em>Proceedings of the IEEE</em> 88 (8): 1279–1296. doi:<a href="http://dx.doi.org/10.1109/5.880084" title="10.1109/5.880084">10.1109/5.880084</a>.</p>
<p>Lee, Li, and R. Rose. 1998. “A Frequency Warping Approach to Speaker Normalization.” <em>Speech and Audio Processing, IEEE Transactions on</em> 6 (1): 49–60. doi:<a href="http://dx.doi.org/10.1109/89.650310" title="10.1109/89.650310">10.1109/89.650310</a>.</p>
<p>Levinson, S., M.Y. Liberman, A. Ljolje, and L.G. Miller. 1989. “Speaker Independent Phonetic Transcription of Fluent Speech for Large Vocabulary Speech Recognition.” In <em>Acoustics, Speech, and Signal Processing, 1989. ICASSP-89., 1989 International Conference on</em>, 441–444 vol.1. doi:<a href="http://dx.doi.org/10.1109/ICASSP.1989.266458" title="10.1109/ICASSP.1989.266458">10.1109/ICASSP.1989.266458</a>.</p>
<p>Saon, G., and Jen-Tzung Chien. 2012. “Large-vocabulary Continuous Speech Recognition Systems: a Look at Some Recent Advances.” <em>Signal Processing Magazine, IEEE</em> 29 (6): 18–33. doi:<a href="http://dx.doi.org/10.1109/MSP.2012.2197156" title="10.1109/MSP.2012.2197156">10.1109/MSP.2012.2197156</a>.</p>
<p>Saon, George, and Mukund Padmanabhan. 1999. “Data-driven Approach to Designing Compound Words for Continuous Speech Recognition.” In <em>IEEE Trans. Speech and Audio Processing</em>, 327–332.</p>
<p>Thelen, E. 1996. “Long Term On-line Speaker Adaptation for Large Vocabulary Dictation.” In <em>Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on</em>, 4:2139–2142 vol.4. doi:<a href="http://dx.doi.org/10.1109/ICSLP.1996.607226" title="10.1109/ICSLP.1996.607226">10.1109/ICSLP.1996.607226</a>.</p>
<p>Wang, Xuerui, Andrew McCallum, and Xing Wei. 2007. “Topical N-grams: Phrase and Topic Discovery, with an Application to Information Retrieval.” In <em>Proceedings of the 2007 Seventh IEEE International Conference on Data Mining</em>, 697–702. ICDM ’07. Washington, DC, USA: IEEE Computer Society. doi:<a href="http://dx.doi.org/10.1109/ICDM.2007.86" title="10.1109/ICDM.2007.86">10.1109/ICDM.2007.86</a>. <a href="http://dx.doi.org/10.1109/ICDM.2007.86" title="http://dx.doi.org/10.1109/ICDM.2007.86">http://dx.doi.org/10.1109/ICDM.2007.86</a>.</p>
<p>Wegmann, S., D. McAllaster, J. Orloff, and B. Peskin. 1996. “Speaker Normalization on Conversational Telephone Speech.” In <em>Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on</em>, 1:339–341 vol. 1. doi:<a href="http://dx.doi.org/10.1109/ICASSP.1996.541101" title="10.1109/ICASSP.1996.541101">10.1109/ICASSP.1996.541101</a>.</p>
</div>
    </div>
    
</div>
</body>
</html>
